{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b8be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "\n",
    "base_url = 'http://116.202.111.229:8000'\n",
    "api_key = 'api-key'\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': 'lnzYdcdRr8RiBniVUAXPZlidQAnhDggd'\n",
    "}\n",
    "response = requests.get(f\"{base_url}/evaluate/hint\", headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd09c618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lipinski Productions is a Pittsburgh-based company dedicated to providing the highest level of media production services in video editing, media conversion, graphic design, web design, and more....'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.22.2)\n",
      "Collecting transformers<5.0.0,>=4.34.0\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (9.0.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.7.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2022.3.15)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.2.1)\n",
      "Installing collected packages: tokenizers, safetensors, transformers, sentence-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.4\n",
      "    Uninstalling transformers-4.27.4:\n",
      "      Successfully uninstalled transformers-4.27.4\n",
      "Successfully installed safetensors-0.4.3 sentence-transformers-2.7.0 tokenizers-0.19.1 transformers-4.40.0\n"
     ]
    }
   ],
   "source": [
    "response.json()['hint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf69e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mihae\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.40.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mihae\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbba32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3024780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(df):\n",
    "    return dict(zip(df['naics_label'], df['description']))\n",
    "\n",
    "\n",
    "def eliminate_multimple_spaces_and_newlines(dict):\n",
    "    for key in dict.keys():\n",
    "        dict[key] = ' '.join(dict[key].split())\n",
    "        # eliminate \\n\n",
    "        dict[key] = dict[key].replace('\\n', ' ')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab794c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_game():\n",
    "# Initialize tokenizer and model\n",
    "    model_id = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model5 = AutoModelForSequenceClassification.from_pretrained(\"checkpoint-6500\")\n",
    "    model4 = AutoModelForSequenceClassification.from_pretrained(\"round4-checkpoint-6500\")\n",
    "    naics = pd.read_excel('Naics3 (label) taxonomy.xlsx')\n",
    "    naics_dict = create_dict(naics)\n",
    "    naics_dict = eliminate_multimple_spaces_and_newlines(naics_dict)\n",
    "    # Load dataset and prepare labels\n",
    "    full_dataset = pd.read_csv(\"round5.csv\")\n",
    "    train_dataset, test_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "    labels = np.array(train_dataset['predicted']).reshape(-1, 1)  # Ensure labels are correctly shaped\n",
    "\n",
    "    # Fit the OneHotEncoder once\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(labels)\n",
    "\n",
    "    model5.eval()\n",
    "    model4.eval()\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    return model5, model4, tokenizer, encoder, naics_dict, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e4fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cb5b1ea1e645bc8190d55b14becd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihae\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mihae\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f783c0e592864f3fb793f6b93078d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda719f8329a40278f07033a14282eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf81d31207b40e7be72712a844a71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbf211c55a64e8b938d2664be3a2f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c1a3dd3e294351afa6ace1170e7dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cd1b7c21234ecea1a53ab747c10ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ba863005994bc090f649059df8ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bf97be59a04c06b71df5f1faf8bb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676419e6074d4608bfd30136313876f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2f4b3354044db8ba7d9e89456d9951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to start the game\n"
     ]
    }
   ],
   "source": [
    "model5, model4, tokenizer, encoder, naics_dict, model = prepare_game()\n",
    "print(\"Ready to start the game\")\n",
    "abstain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94937d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'company_id': 5, 'level': 5, 'hint': 'Social Services & NGOs'}\n",
      "0.5497778058052063\n",
      "200 {'result': 'correct', 'score': 100.0, 'answer': '624 Social Assistance'}\n",
      "1 {'company_id': 5, 'level': 1, 'hint': 'Heart to Heart'}\n",
      "0.4787704050540924\n",
      "200 {'result': 'correct', 'score': 500.0, 'answer': '624 Social Assistance'}\n",
      "2 {'company_id': 5, 'level': 2, 'hint': 'Global Health Information Provider | Humanitarian Services | Disaster Management Services | Direct Patient Care | Medication Distribution | Hygiene Supplies Distribution | Healthcare Access Improvement | Domestic Health Services'}\n",
      "0.4971301853656769\n",
      "200 {'result': 'wrong', 'score': -100.0, 'answer': '624 Social Assistance'}\n",
      "3 {'company_id': 5, 'level': 3, 'hint': 'As a healthcare nonprofit organization, Heart to Heart International invests in Kansas City to create a healthier world.'}\n",
      "0.31104329228401184\n",
      "200 {'result': 'correct', 'score': 300.0, 'answer': '624 Social Assistance'}\n",
      "4 {'company_id': 5, 'level': 4, 'hint': 'Heart to Heart International is a non-profit organization that aims to improve healthcare access in the United States and around the world by ensuring quality care is provided equitably in medically under-resourced communities and disaster situations. They have provided humanitarian service in over 130 countries, shipped over $2.4 billion in aid, and logged more than 1 million volunteer hours. The organization focuses on improving access to health, community health, laboratory programs, and global health. They offer various ways to get involved, including donating medical supplies, donating hygiene kits, and volunteering. Heart to Heart has a history of success in improving healthcare access and has shipped products to various countries.'}\n",
      "0.2294565886259079\n",
      "200 {'result': 'wrong', 'score': -200.0, 'answer': '624 Social Assistance'}\n"
     ]
    }
   ],
   "source": [
    "hint = \"\"\n",
    "for i in range(5):\n",
    "    response = requests.get(f\"{base_url}/evaluate/hint\", headers=headers)\n",
    "    time.sleep(2)\n",
    "    print(i, response.json())\n",
    "    if hint == \"\":\n",
    "        hint = response.json()['hint']\n",
    "    else:\n",
    "        hint = hint + \";\" + response.json()['hint']\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(hint, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value for key, value in inputs.items()}\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs5 = model5(**inputs)\n",
    "        logits5 = outputs5.logits\n",
    "\n",
    "        outputs4 = model4(**inputs)\n",
    "        logits4 = outputs4.logits\n",
    "    # Softmax to get probabilities\n",
    "    probabilities5 = torch.nn.functional.softmax(logits5, dim=-1).cpu().numpy()\n",
    "    probabilities4 = torch.nn.functional.softmax(logits4, dim=-1).cpu().numpy()\n",
    "    predicted_index5 = np.argmax(probabilities5, axis=1)\n",
    "    predicted_one_hot5 = np.zeros(probabilities5.shape)\n",
    "    predicted_one_hot5[np.arange(len(probabilities5)), predicted_index5] = 1\n",
    "    predicted_labels5 = encoder.inverse_transform(predicted_one_hot5)\n",
    "    predicted_labels5 = predicted_labels5[0][0]\n",
    "\n",
    "    predicted_index4 = np.argmax(probabilities4, axis=1)\n",
    "    predicted_one_hot4 = np.zeros(probabilities4.shape)\n",
    "    predicted_one_hot4[np.arange(len(probabilities4)), predicted_index4] = 1\n",
    "    predicted_labels4 = encoder.inverse_transform(predicted_one_hot4)\n",
    "    predicted_labels4 = predicted_labels4[0][0]\n",
    "\n",
    "    # make cosine similarity between the hint and description of the predicted_labels and choose the one with the highest similarity\n",
    "    query_embedding = model.encode(hint)\n",
    "    passage_embedding = model.encode(naics_dict[predicted_labels5])\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, passage_embedding)\n",
    "\n",
    "    passage_embedding2 = model.encode(naics_dict[predicted_labels4])\n",
    "    cosine_scores2 = util.pytorch_cos_sim(query_embedding, passage_embedding2)\n",
    "\n",
    "    if cosine_scores > cosine_scores2:\n",
    "        predicted_labels = predicted_labels5\n",
    "    else:\n",
    "        predicted_labels = predicted_labels4\n",
    "        cosine_scores = cosine_scores2\n",
    "    \n",
    "    print(cosine_scores.item())\n",
    "    if i + 1 == 5 and cosine_scores.item() < 0.4 and abstain == False:\n",
    "        abstain = True\n",
    "        data = {\n",
    "            'answer': 'abstain'\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "        print(response.status_code, response.json())\n",
    "    elif i + 1 == 4 and cosine_scores.item() < 0.35 and abstain == False:\n",
    "        abstain = True\n",
    "        data = {\n",
    "            'answer': 'abstain'\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "        print(response.status_code, response.json())\n",
    "    elif i + 1 == 3 and cosine_scores.item() < 0.3 and abstain == False:\n",
    "        abstain = True\n",
    "        data = {\n",
    "            'answer': 'abstain'\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "        print(response.status_code, response.json())\n",
    "    elif i + 1 == 2 and cosine_scores.item() < 0.25 and abstain == False:\n",
    "        abstain = True\n",
    "        data = {\n",
    "            'answer': 'abstain'\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "        print(response.status_code, response.json())\n",
    "    else:\n",
    "        data = {\n",
    "            'answer': predicted_labels\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "        print(response.status_code, response.json())\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c57a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
